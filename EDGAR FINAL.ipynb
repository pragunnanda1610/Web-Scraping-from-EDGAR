{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Company 10Ks From the SEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the libraries\n",
    "\n",
    "This module will require five libraries- the first is the requests library for making the URL requests; bs4 to parse the files and content; pandas which will be used for taking our cleaned data and giving it structure; re libraries to take care of regular expression; and finally urllib library to fetch, read, open and download from URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping the SEC Query Page"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The company search page allows us to make a specific query for a single company and their filings, and this page will then return all the documents related to that company. From here, we can filter all of their documents to the ones that meet our criteria.\n",
    "\n",
    "This includes being able to filter by a specific date or even a particular type of form. Once, we've filtered the results we can go directly to the document or if we want we can go to the filing folder containing that document. One thing to keep in mind is the scope of your search. If you search for a company name, you can get back more than one company back.\n",
    "\n",
    "This usually doesn't present a problem, but it does mean you may have to look through multiple companies to find the documents you want. It might make more sense to search by the CIK number, to get to the company you want.\n",
    "\n",
    "Link to the company search page: https://www.sec.gov/edgar/searchedgar/companysearch.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section One: Define the Parameters of the Search\n",
    "To create a search we need to \"build\" a URL that takes us to a valid results query, this requires taking our base endpoint and attaching on different parameters to help narrow down our search. I'll do my best to explain how each of these parameters works, but unfortunately, there is no formal documentation on this.\n",
    "\n",
    "Endpoint The endpoint for our EDGAR query is https://www.sec.gov/cgi-bin/browse-edgar if you go to this link without any additional parameters it will be an invalid request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "action: (required) By default should be set to getcompany.\n",
    "CIK: (required) Is the CIK number of the company you are searching.\n",
    "type: (optional) Allows filtering the type of form. For example, if set to 10-k only the 10-K filings are returned.\n",
    "dateb: (optional) Will only return the filings before a given date. The format is as follows YYYYMMDD\n",
    "owner: (required) Is set to exclude by default and specifies ownership. You may also set it to include and only.\n",
    "start: (optional) Is the starting index of the results. For example, if I have 100 results but want to start at 45 of 100, I would pass 45.\n",
    "state: (optional) The company's state.\n",
    "filenum: (optional) The filing number.\n",
    "sic: (optional) The company's SIC (Standard Industry Classification) identifier\n",
    "output: (optional) Defines returned data structure as either xml (atom) or normal html.\n",
    "count: (optional) The number of results you want to see with your request, the max is 100 and if not set it will default to 40.\n",
    "Now that we understand all the parameters let's make a request by defining our endpoint, and then a dictionary of our parameters. Where the key of the dictionary is the parameter name, and the value is the value we want to set for that parameter. Once we've defined these two components we can make our request and parse the response using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# base URL for the SEC EDGAR browser\n",
    "endpoint = r\"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "base_url= r\"https://www.sec.gov\"\n",
    "\n",
    "\n",
    "#Adding Input Box to enter the SIC Code as per client's needs \n",
    "listx = input(\"Enter SIC number(s) separated by space: \")\n",
    "x = listx.split()\n",
    "x.sort()\n",
    "print(\"SIC numbers: \", x)\n",
    "\n",
    "\n",
    "#INPUT BOX FOR 10-K and 10-Q as per client's need\n",
    "listflg = input(\"Enter the type of filing: \")\n",
    "type = listflg.split()\n",
    "print(type,\" filing\")\n",
    "\n",
    "\n",
    "# Input for the time period of the required filings\n",
    "yeari = input(\"Starting period (YYYYMMDD): \")\n",
    "yearf = input(\"Ending period (YYYYMMDD): \")\n",
    "\n",
    "#Looping for the given SIC code to extract the list of companies and their CIK Codes\n",
    "try:\n",
    "    for i in range(0,len(x)):\n",
    "        # define our parameters dictionary\n",
    "        #Paramters: action, SIC code, Count[range(0,100) where 100 displays 100 companies per page]\n",
    "        param_dict = {'action':'getcompany',\n",
    "                      'myowner':'exclude',\n",
    "                      'SIC': (x)[i],\n",
    "                      'count': '100'\n",
    "                     }\n",
    "        try:\n",
    "            q =i,\n",
    "            y=x[i]\n",
    "            print(\"SIC Code- \" + y)\n",
    "            # request the url, and then parse the response.\n",
    "            response = requests.get(url = endpoint, params = param_dict)\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            # Let the user know it was successful.\n",
    "            print('List of Companies from SIC Code '+x[i]+' below-')\n",
    "            print(response.url)\n",
    "            print('Request Successful')\n",
    "            print(\"-\"*110)\n",
    "            company_links= []\n",
    "            company_names=[]\n",
    "\n",
    "        #Parsing the Next Page\n",
    "        #In the example above our results were limited because we did such a narrow search, \n",
    "        #but it's not uncommon for more broad searches to return over 100 different entries.\n",
    "        #In these situations, we can leverage the html parser output to find the link that takes us to the additional results.\n",
    "        #This process is easy; we merely find the link tag that has a rel attribute set to next.\n",
    "        #To demonstrate this, I've added a new URL that will return over 100 results.    \n",
    "\n",
    "            start = int(100)\n",
    "            first_page= [ base_url + tr.td.a['href'] for tr in soup.find_all('tr')[1:] ]\n",
    "\n",
    "           # company_links = company_links + first_page\n",
    "            for j in first_page: \n",
    "                    if j not in company_links: \n",
    "                        company_links.append(j)\n",
    "\n",
    "            while soup.find_all('input',{'type':'button'})!= []:\n",
    "\n",
    "        #       next_page_link = base_url +'/cgi-bin/browse-edgar?action=getcompany&amp;SIC=' + y + '&amp;owner=include&amp;match=&amp;start='+start + '&amp;count=100&amp;hidefilings=0'\n",
    "                str1= '/cgi-bin/browse-edgar?action=getcompany&amp;SIC='\n",
    "                str2= '&amp;owner=include&amp;match=&amp;start='\n",
    "                str3='&amp;count=100&amp;hidefilings=0'\n",
    "\n",
    "                next_page_link = ''.join(map(str, (base_url,str1, y, str2, start,str3)))\n",
    "        #         print(next_page_link)\n",
    "                start= start+100\n",
    "\n",
    "                # request the next page\n",
    "                response = requests.get(url = next_page_link)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                more_companies_list=[ base_url + tr.td.a['href'] for tr in soup.find_all('tr')[1:] ]\n",
    "                company_links= company_links + (more_companies_list)\n",
    "                for k in more_companies_list: \n",
    "                    if k not in company_links: \n",
    "                        company_links.append(k)\n",
    "\n",
    "        #Using the following block of code to remove repetition of same companies which can be expected in the first and last page.            \n",
    "        #This is to avoid downloading similar files again and again.   \n",
    "            final_companies = [] \n",
    "            for f in company_links: \n",
    "                if f not in final_companies: \n",
    "                    final_companies.append(f)\n",
    "\n",
    "\n",
    "        # Get the name of the company in consideration and append it to the list of all companies.\n",
    "\n",
    "            all_company_names=[]\n",
    "\n",
    "            for company_link in final_companies:\n",
    "                company_page=requests.get(company_link)\n",
    "\n",
    "\n",
    "                soup= BeautifulSoup(company_page.content, 'html.parser')\n",
    "                company_name=soup.find('span',{'class':'companyName'}).text\n",
    "                print(company_name)\n",
    "                if company_name not in all_company_names: \n",
    "                    all_company_names.append(company_name)\n",
    "\n",
    "\n",
    "                print('Below is the link to all your selected Filings')\n",
    "                for o in range(0, len(type)):\n",
    "\n",
    "                    # define our parameters dictionary\n",
    "                    param_dict = {'type' : type[o],\n",
    "                                  'dateb': yearf,\n",
    "                                  'datea': yeari,\n",
    "                                  }\n",
    "                    #Above parameters specifies only the format and time-period of filings as required by customer.\n",
    "\n",
    "                    #This block of code will print the company-wise page-url but \n",
    "                    #with the parameters of format and time-period incorporated.\n",
    "                    # request the url, and then parse the response.\n",
    "\n",
    "                    filing_page = requests.get(url = company_link.replace('&hidefilings=0','').replace('&count=40',''), params = param_dict)\n",
    "                    soup = BeautifulSoup(filing_page.content, 'html.parser')\n",
    "                    print(filing_page.url)\n",
    "\n",
    "                    r= soup.find_all('a', {'id' : 'interactiveDataBtn'}) \n",
    "                    document_links= [ base_url + a['href'] for a in r ]\n",
    "                    print('Below are links to your specified form type')\n",
    "\n",
    "\n",
    "                    a=0\n",
    "                    for document_link in document_links:\n",
    "                        document_page=requests.get(document_link)\n",
    "                        soup = BeautifulSoup(document_page.content, 'html.parser')\n",
    "                        print(document_page.url)\n",
    "\n",
    "\n",
    "        #Download the excel files of the specified filing type.\n",
    "\n",
    "                        download_file=[base_url + a['href'] for a in soup.find_all('a',{'class':\"xbrlviewer\"})[:2] ]\n",
    "                        final_download_file= download_file[1]\n",
    "\n",
    "                        outfile_name = company_name + \" \" + str(a)+ \".xls\"\n",
    "                        outfile_name = outfile_name.replace(',','').replace('#','').replace(':','').replace('(see all company filings)','').replace('/','')\n",
    "                        print(outfile_name)\n",
    "                        print(final_download_file)\n",
    "                        print('-'*50)\n",
    "                        urllib.request.urlretrieve(final_download_file, outfile_name)\n",
    "                        a = a+ 1\n",
    "\n",
    "                   \n",
    "                print(\"-\"*110)\n",
    "            i = q,\n",
    "        except:\n",
    "            pass\n",
    "except:\n",
    "    pass\n",
    "print(\"SEARCH AND EXTRACTION COMPLETE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See a list of all companies from one SIC Code.\n",
    "# In case of multiple SIC inputs, this is the list of companies for the last SIC code, in the sorted list.\n",
    "\n",
    "for i in range(len(all_company_names)):\n",
    "    all_company_names[i] = all_company_names[i].replace('(see all company filings)','')\n",
    "    print(all_company_names[i])\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the list of companies into excel file\n",
    "This following block of code is a small practice for downloading the above viewed list as an excel file. This code will download only the companies under one SIC number. In case of multiple SIC no. inputs, the companies under the last SIC will be downloaded. Each SIC can be input individually and the list can be downloaded one after another.                     \n",
    "The excel file has to be processed before usage as raw data. Select the column with the list and click on \"Data\" tab in excel. Under that, select \"Text to Columns\" option. Select \"delimited\" and click on next. Under \"Delimiters\", enter \":\" in the \"Other\" option and click next. In the \"Data Preview\" Space, click on the second box with CIK numbers(it will appear black on selection). Select \"Text\" in the \"Column Data Format\" and click Finish.                                       \n",
    "Now to clean the Companies column, press \"ctrl+F\" from the keyboard or open the \"Find and Replace\" pop up box. Search \"CIK#\" and press \"find all\". In the \"Replace\" tab, do not type anything and just press \"Replace all\".                      \n",
    "Now you have companies with corresponding CIK numbers as usable data.                                           \n",
    "The files will be downloaded in your current working directory(in which the jupyter notebook is running)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the following segment of code to download the list of companies as an MS Excel file.\n",
    "\n",
    "# import xlwt\n",
    "# from tempfile import TemporaryFile\n",
    "# book = xlwt.Workbook()\n",
    "# sheet1 = book.add_sheet('sheet1')\n",
    "\n",
    "\n",
    "# for i,e in enumerate(all_company_names):\n",
    "#     sheet1.write(i,1,e)\n",
    "\n",
    "# name = \"Company_List_2890.xls\"\n",
    "# book.save(name)\n",
    "# book.save(TemporaryFile())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Remarks\n",
    "The EDGAR query system allows us to quickly filter the companies we want to grab filings for and makes the process of finding the forms we need intuitive. With our knowledge of Python and the request system that EDGAR uses we can gain access to a tremendous amount of financial data that is free for public use."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
